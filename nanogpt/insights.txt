GPT MODEL:
1. Input tokenization - convert input text into tokens that the model can process

2. Input embedding - convert tokens into vectors that the model can understand

3. Positional encoding - add information about the position of each token in the sequence

4. Transformer blocks - process the tokens and their relationships
a. LayerNorm - normalize the input
b. Self-Attention - look at previous tokens in sequence to determine the ones to pay more or less attention to
-> multi-head attention - split the input into multiple heads that each compute attention scores, which get concatted
-> initialization diversity, unique positional encoding, orthogonalization enforcement, dropout
c. MLP/FFN - increase detail and complexity for each token individually (for example, information like part of speech,
   subject, etc.). The tokens also learn information like semantic meaning and syntactic role. Broadcast from embedding
   dimension to some bigger dimension

5. Normalize output

6. Linear layer - maps final representations to a vector the size of the vocabulary

7. Softmax - converts the output into probabilities for each token in the vocabulary

8. Next token prediction - predict the next token in the sequence based on the input tokens

TRAINING:
1. Load the dataset

2. Tokenize the dataset

3. Initialize the GPT model

4. Define the loss function (cross-entropy loss (negative log likelihood of correct token))
-> training loss error on trainig data
-> validation loss, new unseen data

IDEAS:

Temperature - how much randomness to add to the model's predictions
-> high temp - logits divided by value > 1, decreasing difference between high and low probabilities
-> low temp - logits divided by value < 1, increasing difference between high and low probabilities

K - how many top tokens to consider when sampling the next token
-> high k - more randomness, more diverse predictions (since sampling from larger pool)
-> low k - less randomness, more predictable predictions (since sampling from smaller pool)

Sequence Length -
-> Padding - add padding tokens to the end of the sequence to make all sequences the same length, then mask during computation
-> Truncation - cut off the end of the sequence if it is too long
-> Segmentation - split the sequence into smaller segments
    -> Overlapping windows - preserve context within the sequence
    -> Special tokens - artificially segment the sequence with special tokens to help the model learn patterns
        -> CLS token - start of the sequence
        -> SEP token - end of the sequence, ends of sentences, etc.
        -> PAD token - padding token
        -> MASK token - mask token (BERT)

Want to max out batch size and sequence length on GPU
-> Powers of 2 are good!
    -> Memory alignment, Cache optimization
    -> Bit operations
    -> Thread balancing
    -> Stability (Balanced binary trees)
        -> Think fast multiplication
        -> Think fast exponentiation

How can we make it faster?

- float 32 - 19.5 tflops
- tensor float 32 - 156 tflops - same exponent range as f32 but lower precision
-> 19 bits same exp size as 32, mantissa size of a 16
-> good compatibility with both (advantage over bfloat)
- float 16 -> 312 tflops

why not int 8? - int 8 evenly distributed, hard to represent normal distributions

ALSO!! saves memory bandwidth (main bottleneck)

Tensor cores - all matrix operations broken up into a 4x4 matrix multiplication, and a sum.
-> 4x4 tiling (costs memory bandwidth)
-> MAC multiplication-and-accumulate
-> -> calculates all 16 of the operations in simultaneous
-> store result (costs memory bandwidth)
-> "strucured sparsity" - toss out 0s
-> mixed precision - 16 bit for weights, 32 bit for activations
    -> hence tfloat 32 - wide dynamic range for stability, but same mantiassa size as 16
    -> we do multiplications in tfloat 32
    -> we accumulate in fp32
    -> we store in fp32
    -> 8x speed (free!!)
- activation functions are super cheap!

-> not actually this fast LMAO but just switching linear layers to tfloat is a 3x speedup!
    -> because memory bandwidth
-> mixed precision is how we can further speed this up
    -> what if our inputs and outputs to our neural network were bfloat16? (torch autocast)
    -> norms are still the same, but we do matrix muls in a lower precision
    -> things like loss are the same since they're more susceptible to multiplication
    -> only 10% further gains (still bottlenecked by memory)

Let's speed it up at compiletime >:)
torch.compile(model)
-> 2x faster totally free!
-> speeds up GPU read/writes
-> analyzes entire thing, and optimizes it, with the knowledge that things don't need to be "eager" (in order)
-> we do things cleverly in advance to stop bottlenecks (like memory bandwidth)
    -> compiles neural net as a single object with no python interpreter (slow)
    -> let's consider the arithmetic 2 + 3 * (input ^ 4)
        -> in traidiontal python, we'd have to call a function, which would call another function, which would call another function
        -> in torch.compile, we can just inline the function
            -> travel time between gpu and gpu memory slows things down a lot
            -> all of these are element wise operations - i'll just make a single trip to the gpu with the entire memory
            -> further, we can fuse these all into a single kernel


Flash attention:
-> mindful of memory hierarchy (it know's whats in high bandwidth memory, shared memory, etc.)
    -> SRAM is much faster
-> doesn't ever materialize nxn attention matrix in hbm
-> faster softmax by subtracting max value from all values

Nice numbers
-> we turned vocab size from 50257 to 50304
All this together, we're over 10 times faster!


FUTURE: photonics??

Insights:
Text quality matters a lot

long sequence length - 1024
> TheYeah old cremdopedEEKik beauty searched trivial Andant propeFor for to alwaysownuke visiting mountains judging after exam letter food In playinglled

shorter sequence length - 128
> The movement was almost imperceptible; it apparently hurt him to
make larger gestures.
"What about the fruit?" I asked him.

> The bus comes at nine. That way I
can get back in time for tomorrow night's work."
"Too bad. It'd be nice

> The Alfa Romeo we rented was a manual drive,
so I was no help at all. Miu did all the driving.

Even with lower loss, short sequence length does better on a small model!
- focused context allows it to train faster
- over small # of total batches, dependencies easier to capture
- finiteness of small model

To what end in robotics?
- task decomposition (set table -> pick up plate, put it on table, pick up napkin, etc.)
- multiimodal transformers (visual + audio)
- paralellization of training
- use llm to build task, prompt, provide loss, cycle, all offline
- adaptive analysis
- build knowledge of a specific domain - i.e. robot learns some embodiment of the home that its in